


# ğŸ§  Running Ollama on LANTA HPC (Transfer Node)

This guide walks you through setting up [Ollama](https://ollama.com) for large language model inference on a LANTA HPC transfer node, with a custom port and local model path setup.

---

## ğŸ“¦ 1. Environment Setup

### â¬‡ï¸ Download and Extract Ollama

```bash
# Login to the LANTA transfer node
mkdir ollama && cd ollama

# Download the Ollama binary for Linux
curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz

# Extract the archive
tar -xvf ollama-linux-amd64.tgz

# Create directory to store models
mkdir models


---

## âš™ï¸ 2. Environment Variables

```bash
# Add Ollama libraries to the dynamic linker path
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:{YOUR_CURRENT_WORKING_DIRECTORY}/lib

# Add Ollama binary to your PATH
export PATH=${PATH}:{YOUR_CURRENT_WORKING_DIRECTORY}/bin

# Set the path to store downloaded models
export OLLAMA_MODELS={YOUR_CURRENT_WORKING_DIRECTORY}/models

# Set a custom host and port for the Ollama server
export OLLAMA_HOST=127.0.0.1:12345
```

> âœ… Tip: Add these exports to your `.bashrc` or `.bash_profile` to persist across sessions.

---

## ğŸš€ 3. Start Ollama Server

```bash
ollama serve &
```

This will launch the Ollama backend server in the background on port `12345`.

---

## ğŸ§  4. Pull and Run a Model

### ğŸ“¥ Pull the LLaMA3 8B model

```bash
ollama pull llama3:8b
```

### â–¶ï¸ Run the model

```bash
ollama run llama3:8b
```

### ğŸ“‹ List available models

```bash
ollama list
```

---

## ğŸ›‘ Stopping the Ollama Server

If you started the server with `&`, find its process:

```bash
ps aux | grep ollama
```

Then stop it with:

```bash
kill <PID>
```

If it doesn't stop:

```bash
kill -9 <PID>
```

---

## ğŸ“ Project Directory Structure

```
ollama/
â”œâ”€â”€ bin/              # Ollama binaries
â”œâ”€â”€ lib/              # Ollama libraries
â”œâ”€â”€ models/           # Local model storage
â”œâ”€â”€ ollama-linux-amd64.tgz
â””â”€â”€ README.md
```

---

## ğŸ“š References

* [Ollama Official Site](https://ollama.com)


---

## âœ… Author

Maintained by **Sakchai Saehoei**
For internal use on the LANTA HPC system.

